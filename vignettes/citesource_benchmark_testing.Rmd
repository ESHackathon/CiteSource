---
title: "Benchmark Testing"

author: ""

date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Benchmark Testing}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  warning = FALSE,
  fig.width = 6,
  fig.height = 6
  )
```
## About this vignette

When estimating he comprehensiveness of a search, researchers often compile a list of relevant studies and evaluate whether or not they are discovered using their search strategy. While benchmarking is an important step in testing the sensitivity of a search, the process can often be a very time consuming. 

This vignette will provide an example of how one may use CiteSource to speed up the process of benchmarking as well as iterate the development of search strings and search strategies. 

## 1. Installation of packages and loading libraries

Use the following code to install CiteSource. Currently, CiteSource lives on GitHub, so you may need to first install the remotes package. This vignette also uses functions from the ggplot2 and dplyr packages.

```{r, results = FALSE, message=FALSE, warning=FALSE}
#Install the remotes packages to enable installation from GitHub
#install.packages("remotes")
#library(remotes)

#Install CiteSource
#remotes::install_github("ESHackathon/CiteSource")

#Load the necessary libraries
library(CiteSource)
library(ggplot2)
library(dplyr)
library(knitr)
```
## 2. Import files from multiple sources

Users can import multiple RIS or bibtex files into CiteSource, which the user can label with source information such as database or platform. 

```{r}
#Import citation files from folder
citation_files <- list.files(path= "benchmark_data", pattern = "\\.ris", full.names = TRUE)
citation_files

#Read in citations and specify sources. Note that labels and strings are not relevant for this use case.
citations <- read_citations(citation_files,
                            cite_sources = c("Benchmark","Search1", "Search2", "Search2", "Search2", "Search2", "Search3", "Search3", "Search3", "Search3", "Search3", "Search4", "Search4", "Search4", "Search5", "Search5", "Search5", "Search5", "Search5","Search5", "Search5", "Search5", "Search6","Search6", "Search6", "Search6" ),
                           cite_labels = c("BenchMark", "search", "search", "search", "search", "search", "search", "search", "search", "search", "search", "search", "search", "search", "search", "search", "search", "search", "search", "search", "search", "search", "search", "search", "search", "search"),
                            tag_naming = "best_guess")
```

## 3. Deduplication and source information

CiteSource allows users to merge duplicates while maintaining information in the cite_source metadata field. Thus, information about the origin of the records is not lost in the deduplication process. The next few steps produce the dataframes that we can use in subsequent analyses.

```{r, results = FALSE, message=FALSE, warning=FALSE}
#Deduplicate citations 
dedup_results <- dedup_citations(citations, merge_citations = TRUE)

#Get unique citations. This yields a dataframe of all records with duplicates merged, but the originating source information maintained in a new variable called cite_source.
unique_citations <- dedup_results$unique

#Count number of unique and non-unique citations from different sources and labels. 
n_unique <- count_unique(unique_citations)

#For each unique citation, determine which sources were present
source_comparison <- compare_sources(unique_citations, comp_type = "sources")

```

## 4. Upset plot to compare discovery of benchmarking articles

An upset plot is a useful for visualizing overlap across multiple sources and provides detail about the number of shared and unique records. Using this data we'll outline a few potential uses of this data, when looking at the discovery of benchmarking articles.

Expanding a list of benchmarking articles:
We have uploaded 55 benchmarking articles. Of these 55 articles we can see that all but 6 have been found across the six searches. We can see the number benchmarking articles that were discovered by each string as well as the number of articles that were shared between searches. 

Looking at the first column we can see that 9 benchmarking articles were found across every search. One may hypthesize that the 140 citations that follow in the second column may have a high number of relevant articles due to the fact that they were also discovered across the 6 searches. If a researcher was interested in building a larger group of benchmarking articles, it's possible that they may want to review these articles first.

Excluding a search due to low precision:
Looking at the plot we can see that search #5 has the largest number of results, well over 6k. Of these, 5,964 are unique to that search. We can also see that search #5 discovers 3 benchmarking articles that would have otherwise not been found had the search not been employed. While a researcher may want to ensure that they capture the highest number of benchmarking articles, the addition of ~6k articles may not be efficient when the result is only 3 benchmarking articles. Instead of including this search in their final strategy, they may consider reviewing the three articles that were found by this search and work to adjust their other searches instead.

Another decision in this case may be to drop search #4 and #6 as each of these strings do not contribute uniquely to the discovery of any benchmarking articles. While the data backs up this decision, there may also be more to consider. For example, if benchmarking articles are bias for any known reason, certain search strategies may be employed with an understanding that benchmarking data may not accurately reflect their potential contribution. (ex. benchmarking articles were gathered from previous systematic reviews that focused on a specific geographic region and the current review is global in nature). 


```{r}
#Generate a source comparison upset plot.
plot_source_overlap_upset(source_comparison, decreasing = c(TRUE, TRUE))

```

## 5. Reviewing the record table

```{r}
unique_citations %>%
  dplyr::filter(stringr::str_detect(cite_label, "BenchMark")) %>%
  record_level_table(return = "DT")

```